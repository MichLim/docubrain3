<!doctype html>
<html lang="en">

<head>
  <title>DocuBrain</title>
  <meta property="og:title" content=Your Project Name" />
  <meta name="twitter:title" content="Your Project Name" />
  <meta name="description" content="Your project about your cool topic described right here." />
  <meta property="og:description" content="Your project about your cool topic described right here." />
  <meta name="twitter:description" content="Your project about your cool topic described right here." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- bootstrap for mobile-friendly layout -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
    integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
    integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
    crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css"
    integrity="sha512-A3o2pGpTV4L/4gr0VfDPL3+zMJyGAawjeJvnNnJ9xh4CjxGBWjv4tw1JPiSy61uc7mDf43NS2S3zEneXcD94zg=="
    crossorigin="anonymous" referrerpolicy="no-referrer" />


</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <h1 class="lead">
        <nobr class="widenobr">DocuBrain</nobr>
        <nobr class="widenobr">For DS 4440</nobr>
      </h1>
    </div>
  </div><!-- end nd-pageheader -->

  <div class="container">
    <div class="row">
      <div class="col justify-content-center text-center">
        <h2>An Analysis of<br>Downstream Transformer Generation of Question-Answer Pairs with Preprocessing and
          Postprocessing Pipelines</h2>
        <br>
        <p>This paper explores automatically generating questions from text and evaluates their
          effectiveness.
          What caught our attention was the prospect of leveraging neural networks to automate a task we've been
          manually performing throughout this semester: crafting questions for assigned articles.
          This sparked our interest in investigating how we could develop a system to streamline and automate this
          process using AQG.</p>
      </div>
    </div>
    <div class="row">
      <div class="col">

        <div class="card">
          <div class="header">
            <h2>Section 1: Introduction</h2>
          </div>
          <div>
            <p>DocuBrain3 is an Automatic Question Generation (AQG) project designed to automatically generate questions
              from provided passages of text.
              By analyzing and understanding the content, our project aims to generate relevant and meaningful questions
              without human intervention.
              It takes passages of text as input and produces corresponding questions, facilitating enhanced
              comprehension
              and utilization of textual content.
              Inspired by the research paper "Downstream Transformer Generation of Question-Answer Pairs with
              Preprocessing
              and Postprocessing Pipelines," our approach leverages the insights from state-of-the-art methodologies
              outlined therein.
              Furthermore, we utilize the SQuAD dataset as a foundational resource, enriching our system's ability to
              generate questions with accuracy and relevance.</p>
          </div>
        </div>

        <div class="card">
          <div class="header">
            <h2>Section 2: Paper Review</h2>
          </div>
          <div>
            <p>"Downstream Transformer Generation of Question-Answer Pairs with Preprocessing and Postprocessing
              Pipelines"
              introduces TP3, a system designed for generating question-answer pairs (QAPs) from given articles.
              The TP3 system utilizes a pretrained T5 transformer model to generate questions, employing preprocessing
              and
              postprocessing pipelines with various NLP tools and algorithms.
              This paper's methodology is very closely related to our project, DocuBrain3, as we also utilize pretrained
              transformer models and the SQuAD dataset to automate question generation from textual content.
              For fine-tuning, each QAP and its context from SQuAD are concatenated, with the question as the target.
              Learning rates are determined using the Cyclical Learning Rates method and adjusted based on evaluation
              metrics like BLEU, ROUGE, METEOR, and BERTScore, leading to the identification of optimal rates. T5-Large
              consistently outperforms T5-Base, with specific learning rates yielding the best results. The paper
              provides
              detailed measurement results for both models, leading to the development of T5-Base-SQuAD1 and
              T5-Large-SQuAD1, which demonstrate superior performance.
            </p>

            <p class="preprocessing-list">
              The preprocessing pipeline in TP3 is crucial for selecting appropriate answers and ultimately generating
              high-quality questions. It involves several steps:
            <ul>
              <li>Removing Unsuitable Sentences: Interrogative and imperative sentences are removed, and semantic-role
                labeling
                is used to analyze and retain sentences with relevant semantic roles.</li>
              <li>Filtering Candidate Answers: Nouns and phrasal nouns are considered candidate answers, with specific
                criteria
                for selection. Named entities and nouns with certain semantic-role labels are retained, while others are
                removed.</li>
              <li>Pruning Inadequate POS Tags: Nouns with inappropriate part-of-speech tags are removed or pruned to
                improve
                question adequacy.</li>
              <li>Removing Common Answers: Words with high probabilities in the Google Books Ngram Dataset, such as
                "anyone"
                and
                "stuff," are removed from candidacy.</li>
              <li>Filtering Answers in Clauses: Candidate answers found within clauses are assessed to ensure their
                relevance.
                Instead of simply removing answers appearing within clauses, our method scrutinizes whether the
                candidate
                answers appear later in the same clause.
                This approach aims to exclude answers that may not contribute effectively to the question generation
                process, leading to more precise filtering.</li>
              <li>Removing Redundant Answers: Redundant candidate answers are removed, prioritizing syntactically
                important
                phrases to avoid inadequate question generation.</li>
            </ul>
            These preprocessing steps ensure that only suitable answers are considered for question generation. For each
            candidate answer identified in the preprocessing stage, three consecutive sentences from the text are
            selected as context. This structured context and corresponding answer is formatted and provided as input to
            a
            fine-tuned T5 model (Text-To-Text Transfer Transformer) which then generates three candidate questions for
            each
            answer. The most semantically
            similar question to thecontext is then chosen as the final target question, ensuring relevance and coherence
            in
            questions.
            </p>

            <p>
              Following question generation, postprocessing steps are employed to refine the quality of generated
              questions. Some examples include removing redundant information and single-word questions. The quality of
              QAPs
              generated by TP3-Base and TP3-Large is evaluated using standard automatic metrics and human judgments.
              T5-SQuAD1 demonstrates superior performance across ROUGE and METEOR metrics compared to other
              models, highlighting its effectiveness in generating high-quality QAPs. In conclusion, TP3 leverages
              pretrained T5 models and robust preprocessing/postprocessing pipelines to generate high-quality QAPs.
              Future
              enhancements could focus on refining transformer models and exploring alternative methods for context
              selection and question generation, advancing the capabilities of QAP generation. We aim to use these
              preprocessing and postprocessing steps and ideas as inspiration for our AQG project.
            </p>
          </div>
        </div>

        <div class="card">
          <div class="header">
            <h2>Section 3: Method & Implementation</h2>
          </div>
          <div>
            <p>
              Preprocessing Steps:<br>
            <ol>
              <li>
                <strong>Data Collection:</strong> We collected the Stanford Question Answering Dataset (SQuAD) for our
                project, which consists of over 87,000 questions sourced from 500+ Wikipedia articles. Each
                question in
                SQuAD is accompanied by a meticulously curated answer, facilitating in-depth exploration and analysis of
                language understanding tasks.
              </li>

              <img src="SQUAD_Dataset.png" alt="SQUAD Dataset.png">
              <br> <br>
              <li>
                <strong>Data Cleaning:</strong>
                <ul>
                  <li>
                    <strong>Remove Unsuitable Sentences from Context:</strong> We filtered out sentences from the
                    context
                    that were interrogative (e.g., "Would you like to have tea or coffee?") or imperative (e.g.,
                    "Remember
                    to pick up the dry cleaning today"), as well as those that did not contain necessary information
                    (e.g.,
                    "Please wait").
                  </li>
                  <li>
                    <strong>Remove Improper Answers (Semantic Role Labels):</strong> We removed improper answers based
                    on
                    Semantic Role Labels (SRL). For example, if the question was "How far did he travel?" and the answer
                    was
                    "Further," where "Further" is labeled as ARGM-EXT (extent), it would be removed.
                  </li>
                  <li>
                    <strong>Remove Improper Answers (Part of Speech):</strong> We filtered out improper answers based on
                    Part of Speech (POS) tags. For instance, if the question was "What time is it?" and the answer was
                    "Now," where "Now" is labeled as a temporal adverb (TMP), it would be removed.
                  </li>
                  <li>
                    <strong>Remove Common Answers:</strong> We excluded common answers such as "stuff," "thing,"
                    "anyone,"
                    etc.
                  </li>
                  <li>
                    <strong>Filtering Answers in Clauses:</strong> Candidate answers found within clauses are evaluated
                    for
                    relevance by considering their placement within the clause. Instead of simply discarding answers
                    appearing within clauses, our method examines whether the candidate answers are located later in the
                    same clause.
                  </li>
                  <li>
                    <strong>Removing Redundant Answers:</strong> Redundant candidate answers are eliminated, giving
                    priority
                    to syntactically significant phrases.
                  </li>

                  <img src="Dependency_Tree.png" alt="Dependency Tree.png">

                </ul>
              </li>
              <li>
                <strong>Tokenization:</strong> The text data was tokenized using the T5 tokenizer, breaking it down into
                individual tokens or words to facilitate further processing.
              </li>
              <li>
                <strong>Input-Output Pair Generation:</strong> We formatted the tokenized data into input-output pairs
                suitable for training the T5 model. This involved concatenating the context and question texts as input
                and
                using the answer texts as the target output. Specifically, we included markers such as "&lt;context&gt;"
                before the actual context and "&lt;answer&gt;" before the actual answer so that the model is made aware
                of
                the different components and is able to distinguish them.
              </li>
            </ol>
            </p>
            <p>
              Experimental Setup:<br>
            <ol>
              <ul>
                <li>
                  <strong>Preprocessed Data vs. Raw Data:</strong> We conducted experiments using both preprocessed and
                  raw
                  SQuAD data to compare the performance of the T5 model under different conditions. The preprocessed
                  data
                  underwent tokenization and input-output pair generation, while the raw data was fed directly into the
                  model without any preprocessing.
                </li>
                <li>
                  <strong>Subset Selection:</strong> Due to time constraints, we opted to run our experiments on a
                  subset of
                  the SQuAD dataset rather than the entire corpus. This subset was selected to ensure a
                  representative sample of the data while minimizing computational resources.
                </li>
              </ul>
            </ol>
            </p>
          </div>
        </div>

        <div class="card">
          <h2>Notebooks</h2>
          <ul>
            <li>
              <a href="https://colab.research.google.com/drive/1uM_XZ9APXoaKjJeEr7cPe2OtzbHASO1x?usp=sharing"
                target="_blank">DB3_Preprocessing.ipynb</a>: This notebook covers all preprocessing steps applied to the
              SQuAD data.
            </li>
            <li>
              <a href="https://colab.research.google.com/drive/1FW68X4s8ysGNMGkl_fAbZKCmxeN2MAtb?usp=sharing"
                target="_blank">DB3_Training.ipynb</a>: This notebook handles the training process of the model.
            </li>
            <li>
              <a href="https://colab.research.google.com/drive/11NgtOrselsw9hNXxs_-_Cvn--pgXGH72?usp=sharing"
                target="_blank">DB3_metrics.ipynb</a>: This notebook contains metrics for preprocessed vs unpreprocessed
              data and details about manual metric strategy.
            </li>
            <li>
              <a href="https://colab.research.google.com/drive/1tpWEF-DAk1Bmwnht7ReeBaVv40Eo-MIk?usp=sharing"
                target="_blank">DB3_Interesting_NN_Visualizations.ipynb</a>: This notebook includes weight
              visualizations, model comparison visualizations, and histograms.
            </li>
          </ul>
        </div>

        <div class="card">
          <div class="header">
            <h2>Section 4: Experimental Findings</h2>

            <p>
              <strong>Model Performance Evaluation</strong>
            <p>
              After conducting extensive experiments with our DocuBrain3 model, we evaluated its performance across
              various metrics. Despite promising initial results, our model encountered challenges in consistently
              generating well-formed questions from given contexts. Figure 6 showcases an example of a properly
              generated question, highlighting instances where the model successfully generated coherent questions.
            </p>
            <img src="properly_generated_q.png" alt="properly_generated_q.png" style="max-width: 80%;">
            <br><br>
            <p>
              However, our analysis revealed that such occurrences were infrequent, with many generated questions
              lacking well-developed answers. Additionally, there were instances where the model produced text in
              languages other than English, as depicted in Figure 7.
            </p>
            <img src="incorrect_generated_q.png" alt="incorrect_generated_q.png" style="max-width: 45%;">

            <br><br>

            <p>
              We hypothesize that the preprocessed model often outputs translations because its weights are much closer to
              the t5-small model, as we are aware that the t5 series of models are very good in machine translation.
              This also may be why it sometimes outputs the context directly, as this is a part of the 'summarization'
              feature of the t5 model. More detailed notes about this can be found in our visualization notebook <a
              href="https://colab.research.google.com/drive/1tpWEF-DAk1Bmwnht7ReeBaVv40Eo-MIk?usp=sharing"
              target="_blank">DB3_Interesting_NN_Visualizations.ipynb</a>.
            </p>

            <p>
              <strong>Metrics Evaluation</strong>
            </p>

            <p>
              In our pursuit of understanding the model's performance, we employed comprehensive metrics evaluation. The
              DB3_Metrics notebook facilitated the comparison between preprocessed and unprocessed data, providing
              insights into the impact of preprocessing on model performance. Line graphs depicting validation loss and
              training loss for both preprocessed and unprocessed data are presented below.
            </p>

            <div class="experimental-findings">

              <div class="image-container">
                <img src="preprocessed_model_loss.png" alt="preprocessed_model_loss.png">
              </div>

              <div class="image-container">
                <img src="unpreprocessed_model_loss.png" alt="unpreprocessed_model_loss.png">
              </div>
            </div>
            <p>
              As outlined in our methodology, we devised a manual metric strategy to evaluate the quality of generated
              questions. By categorizing outputs into distinct groups based on their characteristics, we gained valuable
              insights into the model's strengths and limitations.
            </p>

            <p>
              <strong>Visualization and Analysis</strong>
            </p>

            <p>
              Furthermore, visualizations from the DB3_Interesting_NN_Visualizations notebook shed light on the inner
              workings of our model. Weight visualizations, layer comparisons, and histograms provided valuable insights
              into the model's behavior and training dynamics.
            </p>

            <p>
              Various visualizations are generated by our notebook <a
                href="https://colab.research.google.com/drive/1tpWEF-DAk1Bmwnht7ReeBaVv40Eo-MIk?usp=sharing"
                target="_blank">DB3_Interesting_NN_Visualizations.ipynb</a>. Our goal with creating these visualizations
              was to
              uncover any significant weight distributions between our fine tuned models and our base models.
              More visualizations can be seen within the notebook itself. Some examples are provided below.
            </p>

            <div class="experimental-findings">

              <div class="image-container">
                <p>
                  Fine Tuned Encoder Weights
                </p>
                <img src="fine_tuned_encoder_weights.png" alt="fine_tuned_encoder_weights.png">
              </div>

              <div class="image-container">
                <p>
                  Fine Tuned Decoder Weights
                </p>
                <img src="fine_tuned_decoder_weights.png" alt="fine_tuned_decoder_weights.png">
              </div>
            </div>

            <div class="experimental-findings">

              <div class="image-container">
                <p>
                  t5 Small Encoder Weights
                </p>
                <img src="t5_small_encoder_weights.png" alt="t5_small_encoder_weights.png">
              </div>

              <div class="image-container">
                <p>
                  t5 Small Decoder Weights
                </p>
                <img src="t5_small_decoder_weights.png" alt="t5_small_decoder_weightspng">
              </div>
            </div>

            <p>
              As can be observed by the weight visualizations, there were no significant changes between our fine tuned
              model and the t5-small model. This may be due to our lack of training data and lack of access to resources
              that would allow us to train our models at a faster and more achievable rate. With more training data, we
              believe that we would be able to show a more significant change in weight distribution.
            </p>

            <p>
              Examining the weight distribution alone may not yield substantial insights due to the vast number of
              weights; minor changes may not be discernible in the distribution graph unless they are significant. To
              address this limitation, we opted to visualize the disparities in values between the base model and the
              fine-tuned model to uncover more nuanced observations.

              The visualizations included in the graphs below are as follows, in sequential order:

            </p>

            <ol type="1">
              <li>
                Mean absolute value change in weights for each layer across encoder and decoder blocks.
              </li>
              <li>
                The three layers exhibiting the most considerable differences in weights for both encoder and decoder.
              </li>
              <li>
                The three layers demonstrating the smallest differences in weights for both encoder and decoder.
              </li>
            </ol>

            <img src="mean_diff_per_layer.png" alt="mean_diff_per_layer.png" style="max-width: 30%;">
            <br><br>

            <div class="experimental-findings">
              <div class="image-container">
                <img src="largest_diff_encoder_decoder.png" alt="largest_diff_encoder_decoder.png">
              </div>

              <div class="image-container">
                <img src="smallest_diff_encoder_decoder.png" alt="smallest_diff_encoder_decoder.png">
              </div>
            </div>

            <br>

            <p>
              These analyses were conducted on both t5-base and Zhang Cheng's fine-tuned model, which was derived from
              t5-base.

              The "Mean Differences Per Layer For Encoder/Decoder" histograms provide insights into the overall
              transformation of the
              model during training. By comparing these histograms to an "ideal" state, we can approximate the progress
              made towards achieving that ideal state. Additionally, it offers insights into the distribution of change
              across layers, aiding in assessing their individual contributions.
            </p>

            <p>
              The "Largest Difference for Encoder and Decoder" histogram sheds light on the significance of each layer.
              Significant
              variations in specific layers may suggest the possibility of freezing less crucial layers without
              substantially impacting overall model accuracy or enhancement.

              Similarly, the "Smallest Difference for Encoder and Decoder" histogram serves a comparable purpose to the
              largest difference
              histogram but from the opposite perspective.
            </p>

            <p>
              Overall, our experimental findings underscore the challenges and opportunities inherent in automatic
              question generation. While our model demonstrates potential, further refinement and optimization are
              required to achieve consistent and reliable question generation.

              By integrating insights from our experimental findings, we aim to drive future improvements and
              advancements in the field of automatic question generation. Through future iterative experimentation and
              analysis, we believe the capibilities and performance of our DocuBrain3 model could be greatly improved
              for broader applicability and impact.
            </p>

            </p>
          </div>
        </div>

        <div class="card">
          <h2>Section 5: Conclusion</h2>

          <p>While DocuBrain3 has demonstrated promising capabilities in automatic question generation, further
            improvements could be achieved with access to additional data and computational resources. With a larger and
            more diverse dataset, along with extended training duration, DocuBrain3 could potentially enhance the
            quality and coherence of the questions generated. Increased computing power would enable deeper model
            exploration and refinement, leading to a more nuanced understanding of language patterns and contexts. This
            has the potential to improve the relevance, complexity, and intelligibility of the questions generated,
            ultimately enhancing the overall utility and effectiveness of DocuBrain3 in educational and interactive
            learning settings.
          </p>
          <p>By automating the process of question generation, we seek to provide a valuable tool for enhancing
            learning,
            testing comprehension, and promoting critical thinking.
            The potential applications of this technology range from educational settings, where it can aid students in
            studying and educators in assessing comprehension, to broader societal impacts, such as fostering a culture
            of
            lifelong learning and inquiry.
            The Socratic method, renowned for its emphasis on questioning as a pathway to deeper comprehension and
            critical thinking, serves as our inspiration for this project.
            Moving forward, future work could explore further refinements in question generation algorithms, integration
            with existing educational platforms, and empirical studies to evaluate the effectiveness of our approach in
            enhancing interactive learning outcomes.

          </p>
        </div>


        <div class="card">
          <div class="header">
            <h2>References</h2>
          </div>
          <div>

            <p><a name="Zhang Paper">[1]</a> <a href="https://doi.org/10.48550/arXiv.2205.07387">Cheng Zhang, Hao Zhang,
                Jie
                Wang.
                <em>Downstream Transformer Generation of Question-Answer Pairs with Preprocessing and Postprocessing
                  Pipelines</em></a>
              Downstream Transformer Generation of Question-Answer Pairs with Preprocessing and Postprocessing Pipelines
            </p>

            <p><a name="AQG Paper">[2]</a> <a
                href="https://www.researchgate.net/publication/321147222_Automatic_Question_Generation_Approaches_and_Evaluation_Techniques">Manisha
                Divate and Ambuja Salgaonkar.
                <em>Automatic question generation approaches and evaluation techniques.</em></a>
              Automatic question generation approaches and evaluation techniques.
            </p>

            <p><a name="Socratic Method">[3]</a> <a
                href="https://myjms.mohe.gov.my/index.php/AJUE/article/view/20012">Siti
                Fairuz Dalim, Aina Sakinah Ishak, and Lina Mursyidah Hamzah.
                <em>Promoting Students’ Critical Thinking Through Socratic Method: The Views and Challenges.</em></a>
              Promoting Students’ Critical Thinking Through Socratic Method: The Views and Challenges.
            </p>

            <p><a name="T5 Guide">[4]</a> <a
                href="https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887">
                Fabio Chiusano.
                <em>A Full Guide to Finetuning T5 for Text2Text and Building a Demo with Streamlit.</em>
              </a>
              All you need to know to build a full demo: Hugging Face Hub, Tensorboard, Streamlit, and Hugging Face
              Spaces.
            </p>
          </div>
        </div>

        <h2>Team Members</h2>

        <div class="team-member-container">
          <div class="team-member-card">
            <div>
              <p class="team-member-name">Mihir Dontamsetti</p>
            </div>
            <div class="team-member-links">
              <a href="mailto:dontamsetti.m@northeastern.edu">dontamsetti.m@northeastern.edu</a>
              <br>
              <a href="https://www.linkedin.com/in/mihir-dontamsetti-999b03182/">LinkedIn</a>
            </div>
          </div>

          <div class="team-member-card">
            <div>
              <p class="team-member-name">Michelle Lim</p>
            </div>
            <div class="team-member-links">
              <a href="mailto:lim.miche@northeastern.edu">lim.miche@northeastern.edu</a>
              <br>
              <a href="https://www.linkedin.com/in/mich-lim/">LinkedIn</a>
            </div>
          </div>
        </div>


      </div><!--col-->
    </div><!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>

</body>
<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
  // Google analytics below.
  window.dataLayer = window.dataLayer || [];
</script>

</html>